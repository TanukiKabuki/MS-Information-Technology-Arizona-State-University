{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Harsh Siddhapura\n",
    "# ASU ID: 1230169813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 14: Building the ML Pipeline in SciKit Learn \n",
    "\n",
    "In this lab you will use Scikit Learn to build a machine learning pipeline for a classification application.\n",
    "\n",
    "## Dataset\n",
    "We will be using the US Census, https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a9a dataset for this lab. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Building the Pipeline\n",
    "The pipeline should include two stages:\n",
    "\n",
    "- A standard transformer that transforms the columns of the dataset by giving them a zero mean & standard deviation of 1. \n",
    "- A classifier object such as a Decision Tree \n",
    "\n",
    "An estimator is created using this pipeline then it is used to fit the data. Refer to the make_pipeline()Links to an external site. function documentation on how to create the pipeline.  \n",
    "\n",
    "Run the estimator after building the pipeline. Print the obtained classification accuracy.   \n",
    "\n",
    "The code steps should look like this:\n",
    "\n",
    "- Load the LibSVM file (Don't use read_csv() function).\n",
    "- Create a standard Scaler Object: SS\n",
    "- Create a Decision Tree Object: DT\n",
    "- Create a Pipeline that contains two steps for SS -> DT\n",
    "- Use the Cross Validation Score function & Print the average score \n",
    "\n",
    "Note:\n",
    "The libSVM dataset file we are working with is sparse. The Standard Scaler implementation in SciKit learn is not designed to handle sparse data! Therefore, it will not be able to scale the data in this format.\n",
    "\n",
    "One solution (which is given as a hint in the error messages you might get) is to set with_mean=False and with_std=False. The problem with this solution is that the data will not be standardized! Simply, the standard scaler won't be applied to the data. \n",
    "\n",
    "Another solution is to convert the sparse data into dense data. You can do so by calling the todense() function as follows:\n",
    "\n",
    "- X,y = load_svm_...('a9a')\n",
    "- X_d = X.todense()  \n",
    "\n",
    "Please use the second solution and convert the dataset into a dense matrix before applying the standard scaler to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n",
      "[[-0.49513889 -0.46930197  1.94096624 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197  1.94096624 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " ...\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [ 2.01963532 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]]\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(random_state=0))])\n",
      "\n",
      "\n",
      "Cross Validation score: [0.78826961 0.79407248 0.79914005 0.79453317 0.80451474]\n",
      "\n",
      "Average score: 0.7961060113754724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings \n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Loading the LibSVM file.\n",
    "X, y = load_svmlight_file(\"a9a.txt\")\n",
    "X_d = np.array(X.todense()) # Convert the matrix to a NumPy array\n",
    "\n",
    "# Step 2: Creating standard scaler.\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(X_d))\n",
    "print(scaler.transform(X_d))\n",
    "\n",
    "# Step 3: Creating decision tree object.\n",
    "dec_tree = DecisionTreeClassifier(random_state = 0) \n",
    "\n",
    "# Step 4: Creating a Pipeline that contains two steps for SS -> DT\n",
    "pipeline = make_pipeline(scaler, dec_tree)\n",
    "print(pipeline)\n",
    "\n",
    "# Step 5: Using cross val score to find out average\n",
    "scores = cross_val_score(pipeline, X_d, y, cv = 5) \n",
    "print(\"\\n\\nCross Validation score:\", scores) \n",
    "print(\"\\nAverage score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Parameter Fine Tuning with Cross Validation  \n",
    "\n",
    "In this part you will add a parameter grid to your code to experiment with different parameter values and select the one that gives the highest accuracy. The parameter set is:  \n",
    "- Impurity Measure: “gini”, “entropy”\n",
    "- Maximum Tree Depth: 5, 10, 15, 20  \n",
    "\n",
    "For cross validation, you can use 5 folds.  \n",
    "\n",
    "Check the obtained results. Which parameter set gives the best accuracy?  \n",
    "\n",
    "Notice that you still need to create the pipeline and use it for fitting the model while also performing cross validation & parameter fine tuning.  \n",
    "\n",
    "The code steps should look something similar to this:\n",
    "\n",
    "- Load LibSVM Data\n",
    "- Create a Standard Scalar Object: SS\n",
    "- Create a Decision Tree Object: DT\n",
    "- Create a Pipeline with two steps: SS -> DT\n",
    "- Create a Parameter Grid [['gini', 'entropy'],[5,10,15,20]]\n",
    "- Create the Grid Search Cross Validation object\n",
    "- Call the fit function to fit the pipeline to the data and try different parameter combinations\n",
    "- Print the best obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n",
      "[[-0.49513889 -0.46930197  1.94096624 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197  1.94096624 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " ...\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [ 2.01963532 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]]\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(random_state=0))])\n",
      "\n",
      "\n",
      "Best parameters:  {'decisiontreeclassifier__criterion': 'entropy', 'decisiontreeclassifier__max_depth': 10}\n",
      "\n",
      "Best estimator:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=10,\n",
      "                                        random_state=0))])\n",
      "\n",
      "Best cross-validation score:  0.8351710300812097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import warnings \n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Loading the LibSVM file.\n",
    "X, y = load_svmlight_file(\"a9a.txt\")\n",
    "X_d = np.array(X.todense()) # Convert the matrix to a NumPy array\n",
    "\n",
    "# Step 2: Creating standard scaler.\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(X_d))\n",
    "print(scaler.transform(X_d))\n",
    "\n",
    "# Step 3: Creating decision tree object.\n",
    "dec_tree = DecisionTreeClassifier(random_state = 0) \n",
    "\n",
    "# Step 4: Creating a Pipeline that contains two steps for SS -> DT\n",
    "pipeline = make_pipeline(scaler, dec_tree)\n",
    "print(pipeline)\n",
    "\n",
    "# Step 5: Creating a Parameter Grid\n",
    "param_grid = {'decisiontreeclassifier__criterion': ['gini', 'entropy'],\n",
    "              'decisiontreeclassifier__max_depth': [5, 10, 15, 20]}\n",
    "\n",
    "# Step 6: Creating the Grid Search Cross Validation object\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Step 7: Call the fit function to fit the pipeline to the data and try different parameter combinations\n",
    "grid.fit(X_d, y)\n",
    "\n",
    "# Step 8: Print the best obtained results\n",
    "print(\"\\n\\nBest parameters: \", grid.best_params_)\n",
    "print(\"\\nBest estimator: \", grid.best_estimator_)\n",
    "print(\"\\nBest cross-validation score: \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Extra Credit (5 points): Parameter Fine Tuning, Cross Validation with Train/Test Split  \n",
    "\n",
    "Notice that in parts I & II we did not use a train/test split so the scalar is applied independently to each. That's because the way the cross_val_score() function is implemented does not give access to the learned models. The cross_val_score() function should only be used in cases of comparing between different types of classifiers (DT vs SVM vs ...). The GridSearchCV() object is more suitable for parameter fine tuning. It also gives access to the model that gave the best parameters.\n",
    "\n",
    "In this part, we will modify Part II so that the data is split into train/test parts, the pipeline is fit to the training data with parameter fine tuning and cross validation involved, and finally, the testing data is passed through the scalar, and the best resulting model is applied to the scaled test data to give the final model accuracy.\n",
    "\n",
    "The code steps should look something similar to this:\n",
    "\n",
    "- Load LibSVM Data\n",
    "- Split data into Train and Test sets\n",
    "- Create a Standard Scalar Object: SS\n",
    "- Create a Decision Tree Object: DT\n",
    "- Create a Pipeline with two steps: SS -> DT\n",
    "- Create a Parameter Grid [['gini', 'entropy'],[5,10,15,20]]\n",
    "- Create the Grid Search Cross Validation object\n",
    "- Call the fit function to fit the pipeline to the Train data and try the different parameter combinations\n",
    "- Extract the best model from the Grid Search Cross Validation Object. Check the best_estimator_ attribute.\n",
    "- Apply the Scalar to the Test data\n",
    "- Apply the best model to the scaled Test data\n",
    "- Print the obtained test results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n",
      "[[-0.49513889 -0.46930197  1.94096624 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197  1.94096624 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " ...\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [ 2.01963532 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]\n",
      " [-0.49513889 -0.46930197 -0.51520731 ... -0.03087016 -0.02479131\n",
      "  -0.00554189]]\n",
      "\n",
      "\n",
      "Best parameters:  {'decisiontreeclassifier__criterion': 'gini', 'decisiontreeclassifier__max_depth': 10}\n",
      "\n",
      "Best estimator:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(max_depth=10, random_state=0))])\n",
      "\n",
      "Best cross-validation score:  0.8320025616375615\n",
      "\n",
      "Test score:  0.7658529095654845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "import warnings \n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Loading the LibSVM file.\n",
    "X, y = load_svmlight_file(\"a9a.txt\")\n",
    "X_d = np.array(X.todense()) # Convert the matrix to a NumPy array\n",
    "\n",
    "# Step 2: Split data into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Creating standard scaler.\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(X_d))\n",
    "print(scaler.transform(X_d))\n",
    "\n",
    "# Step 4: Creating decision tree object.\n",
    "dec_tree = DecisionTreeClassifier(random_state = 0) \n",
    "\n",
    "# Step 5: Creating a Pipeline that contains two steps for SS -> DT\n",
    "pipeline = make_pipeline(scaler, dec_tree)\n",
    "\n",
    "# Step 6: Creating a Parameter Grid\n",
    "param_grid = {'decisiontreeclassifier__criterion': ['gini', 'entropy'],\n",
    "              'decisiontreeclassifier__max_depth': [5, 10, 15, 20]}\n",
    "\n",
    "# Step 7: Creating the Grid Search Cross Validation object\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Step 8: Call the fit function to fit the pipeline to the Train data and try the different parameter combinations\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Extract the best model from the Grid Search Cross Validation Object. Check the best_estimator_ attribute.\n",
    "print(\"\\n\\nBest parameters: \", grid.best_params_)\n",
    "print(\"\\nBest estimator: \", grid.best_estimator_)\n",
    "print(\"\\nBest cross-validation score: \", grid.best_score_)\n",
    "\n",
    "# Step 10: Apply the Scalar to the Test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 11: Apply the best model to the scaled Test data\n",
    "test_score = grid.score(X_test_scaled, y_test)\n",
    "\n",
    "# Step 12: Print the obtained test results\n",
    "print(\"\\nTest score: \", test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
